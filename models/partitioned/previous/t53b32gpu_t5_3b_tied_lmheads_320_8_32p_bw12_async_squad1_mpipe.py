"""AutoGenerated with:
python -m autopipe.partition new_t5 --model_name_or_path t5-3b --t5_task squad1 --lmhead --n_iter 10 --analysis_batch_size 8 --partitioning_batch_size 8 --ct new_trace_cache_t53b_320_8_op --cp new_prof_cache_t53b_320_8_op_ftpipe --stateless_tied --lmhead --n_partitions 32 --L 64 --max_seq_length 320 --answer_max_seq_length 8 --partitioning_method mpipe --preset ftpipe --dont_use_async_meta_alg --save_memory_mode --special_blocks T5Block --output_file t53b32gpu_
"""
import math
import torch.fft
import torch.nn.functional
import torch.functional
import torch.linalg
import torch
from torch import Tensor
import torch.nn as nn
from itertools import chain
from typing import Optional, Tuple, Iterator, Iterable, OrderedDict, Dict
import collections

from typing import Type
from torch.nn.modules.linear import Linear
from models.normal.NLP_models.stateless import StatelessEmbedding
from torch.nn.modules.sparse import Embedding
from models.new_t5_example.modeling_t5 import T5LayerNorm
from torch.nn.modules.dropout import Dropout
# this is an auto generated file do not edit unless you know what you are doing


# partition adjacency
# model inputs {0, 24, 47}
# partition 0 {'inputs': {'decoder_input_ids', 'decoder_attention_mask', 'input_ids', 'attention_mask'}, 'outputs': {24, 1}}
# partition 1 {'inputs': {0}, 'outputs': {2}}
# partition 2 {'inputs': {1}, 'outputs': {3}}
# partition 3 {'inputs': {2}, 'outputs': {4}}
# partition 4 {'inputs': {3}, 'outputs': {5}}
# partition 5 {'inputs': {4}, 'outputs': {6}}
# partition 6 {'inputs': {5}, 'outputs': {7}}
# partition 7 {'inputs': {6}, 'outputs': {8}}
# partition 8 {'inputs': {7}, 'outputs': {9}}
# partition 9 {'inputs': {8}, 'outputs': {10}}
# partition 10 {'inputs': {9}, 'outputs': {11}}
# partition 11 {'inputs': {10}, 'outputs': {12}}
# partition 12 {'inputs': {11}, 'outputs': {13}}
# partition 13 {'inputs': {12}, 'outputs': {14}}
# partition 14 {'inputs': {13}, 'outputs': {15}}
# partition 15 {'inputs': {14}, 'outputs': {16}}
# partition 16 {'inputs': {15}, 'outputs': {17}}
# partition 17 {'inputs': {16}, 'outputs': {18}}
# partition 18 {'inputs': {17}, 'outputs': {19}}
# partition 19 {'inputs': {18}, 'outputs': {20}}
# partition 20 {'inputs': {19}, 'outputs': {21}}
# partition 21 {'inputs': {20}, 'outputs': {22}}
# partition 22 {'inputs': {21}, 'outputs': {23}}
# partition 23 {'inputs': {22}, 'outputs': {24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47}}
# partition 24 {'inputs': {0, 'decoder_input_ids', 23, 'attention_mask'}, 'outputs': {25}}
# partition 25 {'inputs': {24, 23}, 'outputs': {26}}
# partition 26 {'inputs': {25, 23}, 'outputs': {27}}
# partition 27 {'inputs': {26, 23}, 'outputs': {28}}
# partition 28 {'inputs': {27, 23}, 'outputs': {29}}
# partition 29 {'inputs': {28, 23}, 'outputs': {30}}
# partition 30 {'inputs': {29, 23}, 'outputs': {31}}
# partition 31 {'inputs': {30, 23}, 'outputs': {32}}
# partition 32 {'inputs': {31, 23}, 'outputs': {33}}
# partition 33 {'inputs': {32, 23}, 'outputs': {34}}
# partition 34 {'inputs': {33, 23}, 'outputs': {35}}
# partition 35 {'inputs': {34, 23}, 'outputs': {36}}
# partition 36 {'inputs': {35, 23}, 'outputs': {37}}
# partition 37 {'inputs': {36, 23}, 'outputs': {38}}
# partition 38 {'inputs': {37, 23}, 'outputs': {39}}
# partition 39 {'inputs': {38, 23}, 'outputs': {40}}
# partition 40 {'inputs': {39, 23}, 'outputs': {41}}
# partition 41 {'inputs': {40, 23}, 'outputs': {42}}
# partition 42 {'inputs': {41, 23}, 'outputs': {43}}
# partition 43 {'inputs': {42, 23}, 'outputs': {44}}
# partition 44 {'inputs': {43, 23}, 'outputs': {45}}
# partition 45 {'inputs': {44, 23}, 'outputs': {46}}
# partition 46 {'inputs': {45, 23}, 'outputs': {47}}
# partition 47 {'inputs': {'labels', 46, 23}, 'outputs': {'output'}}
# model outputs {47}


def create_pipeline_configuration(DEBUG=False, batch_size=8):
    config = {
        'batch_dim': 0,
        'depth': 10000,
        'basic_blocks': (Linear,StatelessEmbedding,Embedding,T5LayerNorm,Dropout),
        'model_inputs': {
            'attention_mask': {
                'shape': torch.Size([8, 320]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0, 24]},
            'decoder_attention_mask': {
                'shape': torch.Size([8, 8]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0]},
            'decoder_input_ids': {
                'shape': torch.Size([8, 8]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0, 24]},
            'input_ids': {
                'shape': torch.Size([8, 320]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [0]},
            'labels': {
                'shape': torch.Size([8, 8]),
                'dtype': torch.int64,
                'is_batched': True,
                'used_by': [47]}},
        'model_outputs': {
            'T5ForConditionalGeneration/torch.nn.functional::cross_entropy_5820': {
                'shape': torch.Size([1]),
                'dtype': torch.float32,
                'is_batched': False,
                'created_by': 47}},
        'stages': {
            0: {
                'stage_cls': Partition0,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([8, 320]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'decoder_attention_mask': {
                        'shape': torch.Size([8, 8]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'decoder_input_ids': {
                        'shape': torch.Size([8, 8]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'input_ids': {
                        'shape': torch.Size([8, 320]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1}},
                'outputs': {
                    'T5ForConditionalGeneration/Parameter[shared_embed_weight]': {
                        'shape': torch.Size([32100, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [24]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/tuple::__getitem___136_0': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/prim::TupleConstruct_145_0': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [1]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/prim::TupleConstruct_145_1': {
                        'shape': None,
                        'dtype': None,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [1]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Size::__getitem___2087': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [24]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___2117': {
                        'shape': torch.Size([8, 1, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [24]}},
                'devices': ['cpu' if DEBUG else 'cuda:0'],
                'stage_depth': 47},
            1: {
                'stage_cls': Partition1,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/tuple::__getitem___136_0': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/prim::TupleConstruct_145_0': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/prim::TupleConstruct_145_1': {
                        'shape': None,
                        'dtype': None,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 0}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___234': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [2]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___236': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [2]}},
                'devices': ['cpu' if DEBUG else 'cuda:1'],
                'stage_depth': 46},
            2: {
                'stage_cls': Partition2,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___234': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___236': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 1}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___318': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [3]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___320': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [3]}},
                'devices': ['cpu' if DEBUG else 'cuda:2'],
                'stage_depth': 45},
            3: {
                'stage_cls': Partition3,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___318': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 2},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___320': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 2}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___402': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [4]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___404': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [4]}},
                'devices': ['cpu' if DEBUG else 'cuda:3'],
                'stage_depth': 44},
            4: {
                'stage_cls': Partition4,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___402': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 3},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___404': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 3}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___486': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [5]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___488': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [5]}},
                'devices': ['cpu' if DEBUG else 'cuda:4'],
                'stage_depth': 43},
            5: {
                'stage_cls': Partition5,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___486': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 4},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___488': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 4}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___570': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [6]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___572': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [6]}},
                'devices': ['cpu' if DEBUG else 'cuda:5'],
                'stage_depth': 42},
            6: {
                'stage_cls': Partition6,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___570': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 5},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___572': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 5}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___654': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [7]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___656': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [7]}},
                'devices': ['cpu' if DEBUG else 'cuda:6'],
                'stage_depth': 41},
            7: {
                'stage_cls': Partition7,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___654': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 6},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___656': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 6}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___738': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [8]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___740': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [8]}},
                'devices': ['cpu' if DEBUG else 'cuda:7'],
                'stage_depth': 40},
            8: {
                'stage_cls': Partition8,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___738': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 7},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___740': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 7}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___822': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [9]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___824': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [9]}},
                'devices': ['cpu' if DEBUG else 'cuda:8'],
                'stage_depth': 39},
            9: {
                'stage_cls': Partition9,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___822': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 8},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___824': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 8}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___906': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [10]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___908': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [10]}},
                'devices': ['cpu' if DEBUG else 'cuda:9'],
                'stage_depth': 38},
            10: {
                'stage_cls': Partition10,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___906': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 9},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___908': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 9}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___990': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___992': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [11]}},
                'devices': ['cpu' if DEBUG else 'cuda:10'],
                'stage_depth': 37},
            11: {
                'stage_cls': Partition11,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___990': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___992': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 10}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1074': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1076': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [12]}},
                'devices': ['cpu' if DEBUG else 'cuda:11'],
                'stage_depth': 36},
            12: {
                'stage_cls': Partition12,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1074': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1076': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 11}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1158': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1160': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [13]}},
                'devices': ['cpu' if DEBUG else 'cuda:12'],
                'stage_depth': 35},
            13: {
                'stage_cls': Partition13,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1158': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 12},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1160': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 12}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1242': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1244': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [14]}},
                'devices': ['cpu' if DEBUG else 'cuda:13'],
                'stage_depth': 34},
            14: {
                'stage_cls': Partition14,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1242': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 13},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1244': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 13}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1326': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1328': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [15]}},
                'devices': ['cpu' if DEBUG else 'cuda:14'],
                'stage_depth': 33},
            15: {
                'stage_cls': Partition15,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1326': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1328': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 14}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1410': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [16]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1412': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [16]}},
                'devices': ['cpu' if DEBUG else 'cuda:15'],
                'stage_depth': 32},
            16: {
                'stage_cls': Partition16,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1410': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 15},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1412': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 15}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1494': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [17]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1496': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [17]}},
                'devices': ['cpu' if DEBUG else 'cuda:16'],
                'stage_depth': 31},
            17: {
                'stage_cls': Partition17,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1494': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 16},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1496': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 16}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1578': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [18]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1580': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [18]}},
                'devices': ['cpu' if DEBUG else 'cuda:17'],
                'stage_depth': 30},
            18: {
                'stage_cls': Partition18,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1578': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 17},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1580': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 17}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1662': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [19]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1664': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [19]}},
                'devices': ['cpu' if DEBUG else 'cuda:18'],
                'stage_depth': 29},
            19: {
                'stage_cls': Partition19,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1662': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 18},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1664': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 18}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1746': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [20]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1748': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [20]}},
                'devices': ['cpu' if DEBUG else 'cuda:19'],
                'stage_depth': 28},
            20: {
                'stage_cls': Partition20,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1746': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 19},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1748': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 19}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1830': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [21]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1832': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [21]}},
                'devices': ['cpu' if DEBUG else 'cuda:20'],
                'stage_depth': 27},
            21: {
                'stage_cls': Partition21,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1830': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 20},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1832': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 20}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1914': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [22]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1916': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [22]}},
                'devices': ['cpu' if DEBUG else 'cuda:21'],
                'stage_depth': 26},
            22: {
                'stage_cls': Partition22,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1914': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 21},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1916': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 21}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1998': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [23]},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___2000': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [23]}},
                'devices': ['cpu' if DEBUG else 'cuda:22'],
                'stage_depth': 25},
            23: {
                'stage_cls': Partition23,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1998': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 22},
                    'T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___2000': {
                        'shape': torch.Size([8, 32, 320, 320]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 22}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_24': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [24]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::__getattribute___2248': {
                        'shape': torch.Size([3]),
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [24]}},
                'devices': ['cpu' if DEBUG else 'cuda:23'],
                'stage_depth': 24},
            24: {
                'stage_cls': Partition24,
                'inputs': {
                    'attention_mask': {
                        'shape': torch.Size([8, 320]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'decoder_input_ids': {
                        'shape': torch.Size([8, 8]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/Parameter[shared_embed_weight]': {
                        'shape': torch.Size([32100, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_24': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 23},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Size::__getitem___2087': {
                        'shape': None,
                        'dtype': int,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___2117': {
                        'shape': torch.Size([8, 1, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 0},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::__getattribute___2248': {
                        'shape': torch.Size([3]),
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 23}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_25': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [25]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2330': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [25]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2332': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [25]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2334': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [25]}},
                'devices': ['cpu' if DEBUG else 'cuda:24'],
                'stage_depth': 23},
            25: {
                'stage_cls': Partition25,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_25': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 24},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2330': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 24},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2332': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 24},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2334': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 24}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_26': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [26]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2481': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [26]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2483': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [26]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2485': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [26]}},
                'devices': ['cpu' if DEBUG else 'cuda:25'],
                'stage_depth': 22},
            26: {
                'stage_cls': Partition26,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_26': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 25},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2481': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 25},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2483': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 25},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2485': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 25}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_27': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [27]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2632': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [27]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2634': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [27]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2636': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [27]}},
                'devices': ['cpu' if DEBUG else 'cuda:26'],
                'stage_depth': 21},
            27: {
                'stage_cls': Partition27,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_27': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 26},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2632': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 26},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2634': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 26},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2636': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 26}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_28': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [28]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2783': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [28]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2785': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [28]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2787': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [28]}},
                'devices': ['cpu' if DEBUG else 'cuda:27'],
                'stage_depth': 20},
            28: {
                'stage_cls': Partition28,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_28': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 27},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2783': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 27},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2785': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 27},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2787': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 27}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_29': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [29]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2934': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [29]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2936': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [29]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2938': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [29]}},
                'devices': ['cpu' if DEBUG else 'cuda:28'],
                'stage_depth': 19},
            29: {
                'stage_cls': Partition29,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_29': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 28},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2934': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 28},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2936': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 28},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2938': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 28}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_30': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [30]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3085': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [30]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3087': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [30]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3089': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [30]}},
                'devices': ['cpu' if DEBUG else 'cuda:29'],
                'stage_depth': 18},
            30: {
                'stage_cls': Partition30,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_30': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 29},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3085': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 29},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3087': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 29},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3089': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 29}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_31': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [31]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3236': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [31]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3238': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [31]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3240': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [31]}},
                'devices': ['cpu' if DEBUG else 'cuda:30'],
                'stage_depth': 17},
            31: {
                'stage_cls': Partition31,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_31': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 30},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3236': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 30},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3238': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 30},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3240': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 30}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_32': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [32]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3387': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [32]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3389': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [32]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3391': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [32]}},
                'devices': ['cpu' if DEBUG else 'cuda:31'],
                'stage_depth': 16},
            32: {
                'stage_cls': Partition32,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_32': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 31},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3387': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 31},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3389': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 31},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3391': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 31}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_33': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [33]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3538': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [33]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3540': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [33]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3542': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [33]}},
                'devices': ['cpu' if DEBUG else 'cuda:32'],
                'stage_depth': 15},
            33: {
                'stage_cls': Partition33,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_33': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 32},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3538': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 32},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3540': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 32},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3542': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 32}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_34': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [34]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3689': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [34]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3691': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [34]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3693': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [34]}},
                'devices': ['cpu' if DEBUG else 'cuda:33'],
                'stage_depth': 14},
            34: {
                'stage_cls': Partition34,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_34': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 33},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3689': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 33},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3691': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 33},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3693': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 33}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_35': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [35]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3840': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [35]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3842': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [35]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3844': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [35]}},
                'devices': ['cpu' if DEBUG else 'cuda:34'],
                'stage_depth': 13},
            35: {
                'stage_cls': Partition35,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_35': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 34},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3840': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 34},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3842': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 34},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3844': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 34}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_36': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [36]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3991': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [36]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3993': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [36]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3995': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [36]}},
                'devices': ['cpu' if DEBUG else 'cuda:35'],
                'stage_depth': 12},
            36: {
                'stage_cls': Partition36,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_36': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 35},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3991': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 35},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3993': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 35},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3995': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 35}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_37': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [37]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4142': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [37]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4144': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [37]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4146': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [37]}},
                'devices': ['cpu' if DEBUG else 'cuda:36'],
                'stage_depth': 11},
            37: {
                'stage_cls': Partition37,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_37': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 36},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4142': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 36},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4144': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 36},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4146': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 36}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_38': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [38]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4293': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [38]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4295': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [38]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4297': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [38]}},
                'devices': ['cpu' if DEBUG else 'cuda:37'],
                'stage_depth': 10},
            38: {
                'stage_cls': Partition38,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_38': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 37},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4293': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 37},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4295': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 37},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4297': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 37}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_39': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [39]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4444': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [39]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [39]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4446': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [39]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4448': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [39]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]': {
                        'shape': torch.Size([8, 8, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [39]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::__getattribute___4451': {
                        'shape': torch.Size([3]),
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [39]}},
                'devices': ['cpu' if DEBUG else 'cuda:38'],
                'stage_depth': 9},
            39: {
                'stage_cls': Partition39,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_39': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 38},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4444': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 38},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 38},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4446': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 38},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4448': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 38},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]': {
                        'shape': torch.Size([8, 8, 4096]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 38},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::__getattribute___4451': {
                        'shape': torch.Size([3]),
                        'dtype': torch.Size,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 38}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_40': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [40]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/tuple::__getitem___4516_0': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [40]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/tuple::__getitem___4580_0': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [40]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/prim::TupleConstruct_4590_0': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [40]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/prim::TupleConstruct_4590_1': {
                        'shape': None,
                        'dtype': None,
                        'req_grad': False,
                        'is_batched': False,
                        'used_by': [40]}},
                'devices': ['cpu' if DEBUG else 'cuda:39'],
                'stage_depth': 8},
            40: {
                'stage_cls': Partition40,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_40': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 39},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/tuple::__getitem___4516_0': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 39},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/tuple::__getitem___4580_0': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 39},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/prim::TupleConstruct_4590_0': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 39},
                    'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/prim::TupleConstruct_4590_1': {
                        'shape': None,
                        'dtype': None,
                        'req_grad': False,
                        'is_batched': False,
                        'created_by': 39}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_41': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [41]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4746': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [41]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4748': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [41]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4750': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [41]}},
                'devices': ['cpu' if DEBUG else 'cuda:40'],
                'stage_depth': 7},
            41: {
                'stage_cls': Partition41,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_41': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 40},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4746': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 40},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4748': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 40},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4750': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 40}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_42': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [42]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4897': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [42]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4899': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [42]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4901': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [42]}},
                'devices': ['cpu' if DEBUG else 'cuda:41'],
                'stage_depth': 6},
            42: {
                'stage_cls': Partition42,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_42': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 41},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4897': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 41},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4899': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 41},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4901': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 41}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_43': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [43]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5048': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [43]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5050': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [43]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5052': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [43]}},
                'devices': ['cpu' if DEBUG else 'cuda:42'],
                'stage_depth': 5},
            43: {
                'stage_cls': Partition43,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_43': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 42},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5048': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 42},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5050': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 42},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5052': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 42}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_44': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [44]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5199': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [44]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5201': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [44]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5203': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [44]}},
                'devices': ['cpu' if DEBUG else 'cuda:43'],
                'stage_depth': 4},
            44: {
                'stage_cls': Partition44,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_44': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 43},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5199': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 43},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5201': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 43},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5203': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 43}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_45': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [45]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5350': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [45]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5352': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [45]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5354': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [45]}},
                'devices': ['cpu' if DEBUG else 'cuda:44'],
                'stage_depth': 3},
            45: {
                'stage_cls': Partition45,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_45': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 44},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5350': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 44},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5352': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 44},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5354': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 44}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_46': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [46]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5501': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [46]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5503': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [46]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5505': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [46]}},
                'devices': ['cpu' if DEBUG else 'cuda:45'],
                'stage_depth': 2},
            46: {
                'stage_cls': Partition46,
                'inputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_46': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 45},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5501': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 45},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5503': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 45},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5505': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 45}},
                'outputs': {
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_47': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [47]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5652': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [47]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5654': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'used_by': [47]},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5656': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'used_by': [47]}},
                'devices': ['cpu' if DEBUG else 'cuda:46'],
                'stage_depth': 1},
            47: {
                'stage_cls': Partition47,
                'inputs': {
                    'labels': {
                        'shape': torch.Size([8, 8]),
                        'dtype': torch.int64,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': -1},
                    'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]_47': {
                        'shape': torch.Size([8, 320, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 46},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5652': {
                        'shape': torch.Size([8, 8, 1024]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 46},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5654': {
                        'shape': torch.Size([8, 32, 8, 8]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': True,
                        'created_by': 46},
                    'T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5656': {
                        'shape': torch.Size([8, 32, 8, 320]),
                        'dtype': torch.float32,
                        'req_grad': False,
                        'is_batched': True,
                        'created_by': 46}},
                'outputs': {
                    'T5ForConditionalGeneration/torch.nn.functional::cross_entropy_5820': {
                        'shape': torch.Size([1]),
                        'dtype': torch.float32,
                        'req_grad': True,
                        'is_batched': False,
                        'used_by': [-1]}},
                'devices': ['cpu' if DEBUG else 'cuda:47'],
                'stage_depth': 0}},
        'stage_to_device_map': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 22, 20, 6, 13, 17, 23, 16, 10, 7, 11, 25, 26, 19, 27, 0, 28, 18, 29, 21, 14, 30, 12, 31]}
    
    
    # switching batch size
    batch_dim = config['batch_dim']
    for d in chain(config['model_inputs'].values(),config['model_outputs'].values()):
        if d['is_batched']:
            shape = d['shape']
            d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    for s in config['stages'].values():
        for d in chain(s['inputs'].values(),s['outputs'].values()):
            if d['is_batched']:
                shape = d['shape']
                d['shape'] = torch.Size(shape[:batch_dim] + (batch_size,) + shape[batch_dim+1:])
    
    return config

class Partition0(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/StatelessEmbedding[embed_tokens]',
            'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Embedding[relative_attention_bias]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
            'T5ForConditionalGeneration/Parameter[shared_embed_weight]',
        ]
    def __init__(self, layers, tensors, device='cuda:0'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'encoder.embed_tokens',
                        'l_1': 'encoder.dropout',
                        'l_2': 'encoder.block.0.layer.0.layer_norm',
                        'l_3': 'encoder.block.0.layer.0.SelfAttention.q',
                        'l_4': 'encoder.block.0.layer.0.SelfAttention.k',
                        'l_5': 'encoder.block.0.layer.0.SelfAttention.v',
                        'l_6': 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias',
                        'l_7': 'encoder.block.0.layer.0.SelfAttention.dropout',
                        'l_8': 'encoder.block.0.layer.0.SelfAttention.o',
                        'l_9': 'encoder.block.0.layer.0.dropout',
                        'l_10': 'encoder.block.0.layer.1.layer_norm',
                        'l_11': 'encoder.block.0.layer.1.DenseReluDense.wi',
                        'l_12': 'encoder.block.0.layer.1.DenseReluDense.dropout',
                        'l_13': 'encoder.block.0.layer.1.DenseReluDense.wo',
                        'l_14': 'encoder.block.0.layer.1.dropout',
                        'p_0': 'shared_embed_weight'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/StatelessEmbedding[embed_tokens] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Embedding[relative_attention_bias] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/Parameter[shared_embed_weight] <=> self.p_0
        # input0 <=> attention_mask
        # input1 <=> decoder_attention_mask
        # input2 <=> decoder_input_ids
        # input3 <=> input_ids
        attention_mask, decoder_attention_mask, decoder_input_ids, input_ids = unflatten(args, self.input_structure)
        t_0 = decoder_input_ids.size()
        t_1 = input_ids.size()
        t_1 = t_1[-1]
        t_1 = input_ids.view(-1, t_1)
        t_1 = self.l_0(self.p_0, t_1)
        t_1 = self.l_1(t_1)
        t_2 = attention_mask[(slice(None, None, None), None, None, slice(None, None, None))]
        t_2 = t_2.to(dtype=torch.float32)
        t_2 = 1.0 - t_2
        t_2 = t_2 * -10000.0
        t_3 = self.l_2(t_1)
        t_4 = self.l_3(t_3)
        t_5 = self.l_4(t_3)
        t_6 = self.l_5(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_7 = t_3[0]
        t_3 = t_3[1]
        t_4 = t_4.view(t_7, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_7, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_6 = t_6.view(t_7, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_5 = t_5.transpose(3, 2)
        t_5 = torch.matmul(t_4, t_5)
        t_4 = torch.arange(t_3, dtype=torch.int64, device=self.device)
        t_4 = t_4[(slice(None, None, None), None)]
        t_3 = torch.arange(t_3, dtype=torch.int64, device=self.device)
        t_3 = t_3[(None, slice(None, None, None))]
        t_4 = t_3 - t_4
        t_3 = torch.abs(t_4)
        t_4 = t_4 > 0
        t_4 = t_4.to(torch.int64)
        t_4 = t_4 * 16
        t_4 = 0 + t_4
        t_8 = t_3.float()
        t_9 = t_3 < 8
        t_8 = t_8 / 8
        t_8 = torch.log(t_8)
        t_10 = math.log(16.0)
        t_10 = t_8 / t_10
        t_10 = t_10 * 8
        t_10 = t_10.to(torch.int64)
        t_10 = 8 + t_10
        t_8 = torch.full_like(t_10, 15, device=self.device)
        t_8 = torch.min(t_10, t_8)
        t_8 = torch.where(t_9, t_3, t_8)
        t_4 += t_8
        t_8 = t_4
        t_8 = t_8.to(self.device)
        t_8 = self.l_6(t_8)
        t_8 = t_8.permute([2, 0, 1])
        t_8 = t_8.unsqueeze(0)
        t_2 = t_8 + t_2
        t_5 += t_2
        t_8 = t_5.float()
        t_8 = torch.nn.functional.softmax(t_8, dim=-1, _stacklevel=3, dtype=None)
        t_5 = t_8.type_as(t_5)
        t_5 = self.l_7(t_5)
        t_6 = torch.matmul(t_5, t_6)
        t_6 = t_6.transpose(1, 2)
        t_6 = t_6.contiguous()
        t_7 = t_6.view(t_7, -1, 4096)
        t_7 = self.l_8(t_7)
        t_6 = self.l_9(t_7)
        t_6 = t_1 + t_6
        t_2 = (t_7, None, t_2)
        t_6 = (t_6,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_6 + t_2
        t_6 = t_2[slice(None, 2, None)]
        t_7 = t_6[0]
        t_1 = self.l_10(t_7)
        t_6 = t_6[1]
        t_2 = t_2[slice(2, None, None)]
        t_1 = self.l_11(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_12(t_1)
        t_1 = self.l_13(t_1)
        t_1 = self.l_14(t_1)
        t_1 = t_7 + t_1
        t_6 = (t_1, t_6)
        t_1 = t_0[-1]
        t_7 = t_0[0]
        t_0 = t_0[1]
        t_5 = torch.arange(t_0, device=self.device)
        t_8 = t_5[(None, None, slice(None, None, None))]
        t_0 = t_8.repeat(t_7, t_0, 1)
        t_5 = t_5[(None, slice(None, None, None), None)]
        t_5 = t_0 <= t_5
        t_0 = decoder_attention_mask.dtype
        t_0 = t_5.to(t_0)
        t_0 = t_0[(slice(None, None, None), None, slice(None, None, None), slice(None, None, None))]
        t_5 = decoder_attention_mask[(slice(None, None, None), None, None, slice(None, None, None))]
        t_5 = t_0 * t_5
        t_5 = t_5.to(dtype=torch.float32)
        t_5 = 1.0 - t_5
        t_5 = t_5 * -10000.0
        # Returning:
        # T5ForConditionalGeneration/Parameter[shared_embed_weight]
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/tuple::__getitem___136
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/prim::TupleConstruct_145
        # T5ForConditionalGeneration/T5Stack[decoder]/Size::__getitem___2087
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___2117
        return list(flatten((self.p_0, t_2, t_6, t_1, t_5)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition1(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:1'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [(1,), (1, 1)]
        self.lookup = {'l_0': 'encoder.block.1.layer.0.layer_norm',
                        'l_1': 'encoder.block.1.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.1.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.1.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.1.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.1.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.1.layer.0.dropout',
                        'l_7': 'encoder.block.1.layer.1.layer_norm',
                        'l_8': 'encoder.block.1.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.1.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.1.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.1.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/tuple::__getitem___136 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/prim::TupleConstruct_145 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = x1 + x0
        t_1 = t_0[slice(None, 2, None)]
        t_1 = t_1[0]
        t_2 = self.l_0(t_1)
        t_0 = t_0[2]
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += t_0
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = t_1 + t_5
        t_0 = (t_2, None, t_0)
        t_5 = (t_5,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_5 + t_0
        t_5 = t_0[slice(None, 2, None)]
        t_2 = t_5[0]
        t_1 = self.l_7(t_2)
        t_5 = t_5[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_5 = (t_1, t_5)
        t_0 = t_5 + t_0
        t_5 = t_0[slice(None, 2, None)]
        t_5 = t_5[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___234
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___236
        return list(flatten((t_5, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition2(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:2'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.2.layer.0.layer_norm',
                        'l_1': 'encoder.block.2.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.2.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.2.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.2.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.2.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.2.layer.0.dropout',
                        'l_7': 'encoder.block.2.layer.1.layer_norm',
                        'l_8': 'encoder.block.2.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.2.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.2.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.2.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___234 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___236 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___318
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___320
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition3(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:3'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.3.layer.0.layer_norm',
                        'l_1': 'encoder.block.3.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.3.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.3.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.3.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.3.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.3.layer.0.dropout',
                        'l_7': 'encoder.block.3.layer.1.layer_norm',
                        'l_8': 'encoder.block.3.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.3.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.3.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.3.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___318 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___320 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___402
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___404
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition4(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:4'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.4.layer.0.layer_norm',
                        'l_1': 'encoder.block.4.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.4.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.4.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.4.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.4.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.4.layer.0.dropout',
                        'l_7': 'encoder.block.4.layer.1.layer_norm',
                        'l_8': 'encoder.block.4.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.4.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.4.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.4.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___402 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___404 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___486
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___488
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition5(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:5'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.5.layer.0.layer_norm',
                        'l_1': 'encoder.block.5.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.5.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.5.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.5.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.5.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.5.layer.0.dropout',
                        'l_7': 'encoder.block.5.layer.1.layer_norm',
                        'l_8': 'encoder.block.5.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.5.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.5.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.5.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___486 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___488 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___570
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___572
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition6(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:6'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.6.layer.0.layer_norm',
                        'l_1': 'encoder.block.6.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.6.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.6.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.6.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.6.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.6.layer.0.dropout',
                        'l_7': 'encoder.block.6.layer.1.layer_norm',
                        'l_8': 'encoder.block.6.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.6.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.6.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.6.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___570 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___572 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___654
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___656
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition7(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:7'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.7.layer.0.layer_norm',
                        'l_1': 'encoder.block.7.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.7.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.7.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.7.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.7.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.7.layer.0.dropout',
                        'l_7': 'encoder.block.7.layer.1.layer_norm',
                        'l_8': 'encoder.block.7.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.7.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.7.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.7.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___654 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___656 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___738
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___740
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition8(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:8'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.8.layer.0.layer_norm',
                        'l_1': 'encoder.block.8.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.8.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.8.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.8.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.8.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.8.layer.0.dropout',
                        'l_7': 'encoder.block.8.layer.1.layer_norm',
                        'l_8': 'encoder.block.8.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.8.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.8.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.8.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___738 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___740 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___822
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___824
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition9(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:9'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.9.layer.0.layer_norm',
                        'l_1': 'encoder.block.9.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.9.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.9.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.9.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.9.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.9.layer.0.dropout',
                        'l_7': 'encoder.block.9.layer.1.layer_norm',
                        'l_8': 'encoder.block.9.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.9.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.9.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.9.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___822 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___824 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___906
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___908
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition10(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:10'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.10.layer.0.layer_norm',
                        'l_1': 'encoder.block.10.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.10.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.10.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.10.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.10.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.10.layer.0.dropout',
                        'l_7': 'encoder.block.10.layer.1.layer_norm',
                        'l_8': 'encoder.block.10.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.10.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.10.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.10.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___906 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___908 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___990
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___992
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition11(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:11'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.11.layer.0.layer_norm',
                        'l_1': 'encoder.block.11.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.11.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.11.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.11.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.11.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.11.layer.0.dropout',
                        'l_7': 'encoder.block.11.layer.1.layer_norm',
                        'l_8': 'encoder.block.11.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.11.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.11.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.11.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___990 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___992 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1074
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1076
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition12(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:12'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.12.layer.0.layer_norm',
                        'l_1': 'encoder.block.12.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.12.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.12.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.12.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.12.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.12.layer.0.dropout',
                        'l_7': 'encoder.block.12.layer.1.layer_norm',
                        'l_8': 'encoder.block.12.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.12.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.12.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.12.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1074 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1076 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1158
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1160
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition13(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:13'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.13.layer.0.layer_norm',
                        'l_1': 'encoder.block.13.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.13.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.13.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.13.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.13.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.13.layer.0.dropout',
                        'l_7': 'encoder.block.13.layer.1.layer_norm',
                        'l_8': 'encoder.block.13.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.13.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.13.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.13.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1158 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1160 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1242
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1244
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition14(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:14'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.14.layer.0.layer_norm',
                        'l_1': 'encoder.block.14.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.14.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.14.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.14.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.14.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.14.layer.0.dropout',
                        'l_7': 'encoder.block.14.layer.1.layer_norm',
                        'l_8': 'encoder.block.14.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.14.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.14.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.14.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1242 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1244 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1326
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1328
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition15(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:15'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.15.layer.0.layer_norm',
                        'l_1': 'encoder.block.15.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.15.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.15.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.15.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.15.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.15.layer.0.dropout',
                        'l_7': 'encoder.block.15.layer.1.layer_norm',
                        'l_8': 'encoder.block.15.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.15.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.15.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.15.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1326 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1328 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1410
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1412
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition16(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:16'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.16.layer.0.layer_norm',
                        'l_1': 'encoder.block.16.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.16.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.16.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.16.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.16.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.16.layer.0.dropout',
                        'l_7': 'encoder.block.16.layer.1.layer_norm',
                        'l_8': 'encoder.block.16.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.16.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.16.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.16.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1410 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1412 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1494
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1496
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition17(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:17'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.17.layer.0.layer_norm',
                        'l_1': 'encoder.block.17.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.17.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.17.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.17.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.17.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.17.layer.0.dropout',
                        'l_7': 'encoder.block.17.layer.1.layer_norm',
                        'l_8': 'encoder.block.17.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.17.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.17.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.17.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1494 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1496 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1578
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1580
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition18(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:18'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.18.layer.0.layer_norm',
                        'l_1': 'encoder.block.18.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.18.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.18.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.18.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.18.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.18.layer.0.dropout',
                        'l_7': 'encoder.block.18.layer.1.layer_norm',
                        'l_8': 'encoder.block.18.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.18.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.18.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.18.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1578 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1580 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1662
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1664
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition19(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:19'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.19.layer.0.layer_norm',
                        'l_1': 'encoder.block.19.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.19.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.19.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.19.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.19.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.19.layer.0.dropout',
                        'l_7': 'encoder.block.19.layer.1.layer_norm',
                        'l_8': 'encoder.block.19.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.19.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.19.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.19.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1662 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1664 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1746
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1748
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition20(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:20'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.20.layer.0.layer_norm',
                        'l_1': 'encoder.block.20.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.20.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.20.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.20.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.20.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.20.layer.0.dropout',
                        'l_7': 'encoder.block.20.layer.1.layer_norm',
                        'l_8': 'encoder.block.20.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.20.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.20.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.20.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1746 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1748 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1830
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1832
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition21(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:21'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.21.layer.0.layer_norm',
                        'l_1': 'encoder.block.21.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.21.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.21.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.21.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.21.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.21.layer.0.dropout',
                        'l_7': 'encoder.block.21.layer.1.layer_norm',
                        'l_8': 'encoder.block.21.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.21.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.21.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.21.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1830 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1832 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1914
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1916
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition22(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:22'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.22.layer.0.layer_norm',
                        'l_1': 'encoder.block.22.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.22.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.22.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.22.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.22.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.22.layer.0.dropout',
                        'l_7': 'encoder.block.22.layer.1.layer_norm',
                        'l_8': 'encoder.block.22.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.22.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.22.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.22.layer.1.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1914 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1916 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_3 = t_3[0]
        t_0 = t_0[2]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1998
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___2000
        return list(flatten((t_3, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition23(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[encoder]/T5LayerNorm[final_layer_norm]',
            'T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:23'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1]
        self.lookup = {'l_0': 'encoder.block.23.layer.0.layer_norm',
                        'l_1': 'encoder.block.23.layer.0.SelfAttention.q',
                        'l_2': 'encoder.block.23.layer.0.SelfAttention.k',
                        'l_3': 'encoder.block.23.layer.0.SelfAttention.v',
                        'l_4': 'encoder.block.23.layer.0.SelfAttention.dropout',
                        'l_5': 'encoder.block.23.layer.0.SelfAttention.o',
                        'l_6': 'encoder.block.23.layer.0.dropout',
                        'l_7': 'encoder.block.23.layer.1.layer_norm',
                        'l_8': 'encoder.block.23.layer.1.DenseReluDense.wi',
                        'l_9': 'encoder.block.23.layer.1.DenseReluDense.dropout',
                        'l_10': 'encoder.block.23.layer.1.DenseReluDense.wo',
                        'l_11': 'encoder.block.23.layer.1.dropout',
                        'l_12': 'encoder.final_layer_norm',
                        'l_13': 'encoder.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[encoder]/T5LayerNorm[final_layer_norm] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1998 <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___2000 <=> x1
        x0, x1 = unflatten(args, self.input_structure)
        t_0 = self.l_0(x0)
        t_1 = self.l_1(t_0)
        t_2 = self.l_2(t_0)
        t_3 = self.l_3(t_0)
        t_0 = t_0.shape
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_1 = t_1.view(t_0, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_2 = t_2.view(t_0, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_0, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_2 = t_2.transpose(3, 2)
        t_2 = torch.matmul(t_1, t_2)
        t_2 += x1
        t_1 = t_2.float()
        t_1 = torch.nn.functional.softmax(t_1, dim=-1, _stacklevel=3, dtype=None)
        t_2 = t_1.type_as(t_2)
        t_2 = self.l_4(t_2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 = t_3.transpose(1, 2)
        t_3 = t_3.contiguous()
        t_0 = t_3.view(t_0, -1, 4096)
        t_0 = self.l_5(t_0)
        t_3 = self.l_6(t_0)
        t_3 = x0 + t_3
        t_0 = (t_0, None, x1)
        t_3 = (t_3,)
        t_0 = t_0[slice(1, None, None)]
        t_0 = t_3 + t_0
        t_3 = t_0[slice(None, 2, None)]
        t_2 = t_3[0]
        t_1 = self.l_7(t_2)
        t_3 = t_3[1]
        t_0 = t_0[slice(2, None, None)]
        t_1 = self.l_8(t_1)
        t_1 = torch.nn.functional.relu(t_1, inplace=False)
        t_1 = self.l_9(t_1)
        t_1 = self.l_10(t_1)
        t_1 = self.l_11(t_1)
        t_1 = t_2 + t_1
        t_3 = (t_1, t_3)
        t_0 = t_3 + t_0
        t_0 = t_0[slice(None, 2, None)]
        t_0 = t_0[0]
        t_0 = self.l_12(t_0)
        t_0 = self.l_13(t_0)
        t_3 = t_0.shape
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::__getattribute___2248
        return list(flatten((t_0, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition24(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/StatelessEmbedding[embed_tokens]',
            'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Embedding[relative_attention_bias]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:24'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.embed_tokens',
                        'l_1': 'decoder.dropout',
                        'l_2': 'decoder.block.0.layer.0.layer_norm',
                        'l_3': 'decoder.block.0.layer.0.SelfAttention.q',
                        'l_4': 'decoder.block.0.layer.0.SelfAttention.k',
                        'l_5': 'decoder.block.0.layer.0.SelfAttention.v',
                        'l_6': 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias',
                        'l_7': 'decoder.block.0.layer.0.SelfAttention.dropout',
                        'l_8': 'decoder.block.0.layer.0.SelfAttention.o',
                        'l_9': 'decoder.block.0.layer.0.dropout',
                        'l_10': 'decoder.block.0.layer.1.layer_norm',
                        'l_11': 'decoder.block.0.layer.1.EncDecAttention.q',
                        'l_12': 'decoder.block.0.layer.1.EncDecAttention.k',
                        'l_13': 'decoder.block.0.layer.1.EncDecAttention.v',
                        'l_14': 'decoder.block.0.layer.1.EncDecAttention.dropout',
                        'l_15': 'decoder.block.0.layer.1.EncDecAttention.o',
                        'l_16': 'decoder.block.0.layer.1.dropout',
                        'l_17': 'decoder.block.0.layer.2.layer_norm',
                        'l_18': 'decoder.block.0.layer.2.DenseReluDense.wi',
                        'l_19': 'decoder.block.0.layer.2.DenseReluDense.dropout',
                        'l_20': 'decoder.block.0.layer.2.DenseReluDense.wo',
                        'l_21': 'decoder.block.0.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/StatelessEmbedding[embed_tokens] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Embedding[relative_attention_bias] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_21
        # input0 <=> attention_mask
        # input2 <=> decoder_input_ids
        # T5ForConditionalGeneration/Parameter[shared_embed_weight] <=> x0
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/Size::__getitem___2087 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/Tensor::__mul___2117 <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[0]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Tensor::__getattribute___2248 <=> x4
        attention_mask, decoder_input_ids, x0, x1, x2, x3, x4 = unflatten(args, self.input_structure)
        t_0 = self.l_12(x1)
        t_1 = self.l_13(x1)
        t_2 = decoder_input_ids.view(-1, x2)
        t_2 = self.l_0(x0, t_2)
        t_2 = self.l_1(t_2)
        t_3 = attention_mask[(slice(None, None, None), None, None, slice(None, None, None))]
        t_3 = t_3.to(dtype=torch.float32)
        t_3 = 1.0 - t_3
        t_3 = t_3 * -1000000000.0
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_4)
        t_6 = self.l_4(t_4)
        t_7 = self.l_5(t_4)
        t_4 = t_4.shape
        t_4 = t_4[slice(None, 2, None)]
        t_8 = t_4[0]
        t_4 = t_4[1]
        t_5 = t_5.view(t_8, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_6 = t_6.view(t_8, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_7 = t_7.view(t_8, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_6 = t_6.transpose(3, 2)
        t_6 = torch.matmul(t_5, t_6)
        t_5 = torch.arange(t_4, dtype=torch.int64, device=self.device)
        t_5 = t_5[(slice(None, None, None), None)]
        t_4 = torch.arange(t_4, dtype=torch.int64, device=self.device)
        t_4 = t_4[(None, slice(None, None, None))]
        t_5 = t_4 - t_5
        t_4 = torch.zeros_like(t_5, device=self.device)
        t_4 = torch.min(t_5, t_4)
        t_4 = -t_4
        t_5 = t_4.float()
        t_9 = t_4 < 16
        t_5 = t_5 / 16
        t_5 = torch.log(t_5)
        t_10 = math.log(8.0)
        t_10 = t_5 / t_10
        t_10 = t_10 * 16
        t_10 = t_10.to(torch.int64)
        t_10 = 16 + t_10
        t_5 = torch.full_like(t_10, 31, device=self.device)
        t_5 = torch.min(t_10, t_5)
        t_5 = torch.where(t_9, t_4, t_5)
        t_5 = 0 + t_5
        t_5 = t_5.to(self.device)
        t_5 = self.l_6(t_5)
        t_5 = t_5.permute([2, 0, 1])
        t_5 = t_5.unsqueeze(0)
        t_5 = t_5 + x3
        t_6 += t_5
        t_4 = t_6.float()
        t_4 = torch.nn.functional.softmax(t_4, dim=-1, _stacklevel=3, dtype=None)
        t_6 = t_4.type_as(t_6)
        t_6 = self.l_7(t_6)
        t_7 = torch.matmul(t_6, t_7)
        t_7 = t_7.transpose(1, 2)
        t_7 = t_7.contiguous()
        t_8 = t_7.view(t_8, -1, 4096)
        t_8 = self.l_8(t_8)
        t_7 = self.l_9(t_8)
        t_7 = t_2 + t_7
        t_5 = (t_8, None, t_5)
        t_7 = (t_7,)
        t_5 = t_5[slice(1, None, None)]
        t_5 = t_7 + t_5
        t_7 = t_5[slice(None, 2, None)]
        t_8 = t_7[0]
        t_2 = self.l_10(t_8)
        t_7 = t_7[1]
        t_5 = t_5[slice(2, None, None)]
        t_6 = self.l_11(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_4 = t_2[0]
        t_2 = t_2[1]
        t_9 = x4[1]
        t_6 = t_6.view(t_4, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_4, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_4, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_6 = t_0.dtype
        t_9 = (1, 32, t_2, t_9)
        t_6 = torch.zeros(t_9, device=self.device, dtype=t_6)
        t_3 = t_6 + t_3
        t_0 += t_3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_14(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_4 = t_1.view(t_4, -1, 4096)
        t_4 = self.l_15(t_4)
        t_1 = self.l_16(t_4)
        t_1 = t_8 + t_1
        t_3 = (t_4, None, t_3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_17(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_5 + t_3
        t_4 = self.l_18(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_19(t_4)
        t_4 = self.l_20(t_4)
        t_4 = self.l_21(t_4)
        t_4 = t_1 + t_4
        t_7 = (t_4, t_7)
        t_3 = t_7 + t_3
        t_7 = t_3[slice(None, 2, None)]
        t_7 = t_7[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2330
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2332
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2334
        return list(flatten((x1, t_7, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition25(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:25'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.1.layer.0.layer_norm',
                        'l_1': 'decoder.block.1.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.1.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.1.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.1.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.1.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.1.layer.0.dropout',
                        'l_7': 'decoder.block.1.layer.1.layer_norm',
                        'l_8': 'decoder.block.1.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.1.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.1.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.1.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.1.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.1.layer.1.dropout',
                        'l_14': 'decoder.block.1.layer.2.layer_norm',
                        'l_15': 'decoder.block.1.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.1.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.1.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.1.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[1]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2330 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2332 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2334 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2481
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2483
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2485
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition26(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:26'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.2.layer.0.layer_norm',
                        'l_1': 'decoder.block.2.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.2.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.2.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.2.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.2.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.2.layer.0.dropout',
                        'l_7': 'decoder.block.2.layer.1.layer_norm',
                        'l_8': 'decoder.block.2.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.2.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.2.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.2.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.2.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.2.layer.1.dropout',
                        'l_14': 'decoder.block.2.layer.2.layer_norm',
                        'l_15': 'decoder.block.2.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.2.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.2.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.2.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[2]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2481 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2483 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2485 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2632
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2634
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2636
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition27(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:27'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.3.layer.0.layer_norm',
                        'l_1': 'decoder.block.3.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.3.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.3.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.3.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.3.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.3.layer.0.dropout',
                        'l_7': 'decoder.block.3.layer.1.layer_norm',
                        'l_8': 'decoder.block.3.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.3.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.3.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.3.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.3.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.3.layer.1.dropout',
                        'l_14': 'decoder.block.3.layer.2.layer_norm',
                        'l_15': 'decoder.block.3.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.3.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.3.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.3.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[3]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2632 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2634 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2636 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2783
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2785
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2787
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition28(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:28'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.4.layer.0.layer_norm',
                        'l_1': 'decoder.block.4.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.4.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.4.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.4.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.4.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.4.layer.0.dropout',
                        'l_7': 'decoder.block.4.layer.1.layer_norm',
                        'l_8': 'decoder.block.4.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.4.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.4.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.4.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.4.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.4.layer.1.dropout',
                        'l_14': 'decoder.block.4.layer.2.layer_norm',
                        'l_15': 'decoder.block.4.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.4.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.4.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.4.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[4]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2783 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2785 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2787 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2934
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2936
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2938
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition29(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:29'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.5.layer.0.layer_norm',
                        'l_1': 'decoder.block.5.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.5.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.5.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.5.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.5.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.5.layer.0.dropout',
                        'l_7': 'decoder.block.5.layer.1.layer_norm',
                        'l_8': 'decoder.block.5.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.5.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.5.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.5.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.5.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.5.layer.1.dropout',
                        'l_14': 'decoder.block.5.layer.2.layer_norm',
                        'l_15': 'decoder.block.5.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.5.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.5.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.5.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[5]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2934 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2936 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2938 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3085
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3087
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3089
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition30(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:30'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.6.layer.0.layer_norm',
                        'l_1': 'decoder.block.6.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.6.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.6.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.6.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.6.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.6.layer.0.dropout',
                        'l_7': 'decoder.block.6.layer.1.layer_norm',
                        'l_8': 'decoder.block.6.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.6.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.6.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.6.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.6.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.6.layer.1.dropout',
                        'l_14': 'decoder.block.6.layer.2.layer_norm',
                        'l_15': 'decoder.block.6.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.6.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.6.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.6.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[6]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3085 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3087 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3089 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3236
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3238
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3240
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition31(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:31'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.7.layer.0.layer_norm',
                        'l_1': 'decoder.block.7.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.7.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.7.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.7.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.7.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.7.layer.0.dropout',
                        'l_7': 'decoder.block.7.layer.1.layer_norm',
                        'l_8': 'decoder.block.7.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.7.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.7.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.7.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.7.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.7.layer.1.dropout',
                        'l_14': 'decoder.block.7.layer.2.layer_norm',
                        'l_15': 'decoder.block.7.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.7.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.7.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.7.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[7]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3236 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3238 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3240 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3387
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3389
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3391
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition32(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:32'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.8.layer.0.layer_norm',
                        'l_1': 'decoder.block.8.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.8.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.8.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.8.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.8.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.8.layer.0.dropout',
                        'l_7': 'decoder.block.8.layer.1.layer_norm',
                        'l_8': 'decoder.block.8.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.8.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.8.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.8.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.8.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.8.layer.1.dropout',
                        'l_14': 'decoder.block.8.layer.2.layer_norm',
                        'l_15': 'decoder.block.8.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.8.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.8.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.8.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[8]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3387 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3389 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3391 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3538
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3540
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3542
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition33(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:33'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.9.layer.0.layer_norm',
                        'l_1': 'decoder.block.9.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.9.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.9.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.9.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.9.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.9.layer.0.dropout',
                        'l_7': 'decoder.block.9.layer.1.layer_norm',
                        'l_8': 'decoder.block.9.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.9.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.9.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.9.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.9.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.9.layer.1.dropout',
                        'l_14': 'decoder.block.9.layer.2.layer_norm',
                        'l_15': 'decoder.block.9.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.9.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.9.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.9.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[9]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3538 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3540 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3542 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3689
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3691
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3693
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition34(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:34'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.10.layer.0.layer_norm',
                        'l_1': 'decoder.block.10.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.10.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.10.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.10.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.10.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.10.layer.0.dropout',
                        'l_7': 'decoder.block.10.layer.1.layer_norm',
                        'l_8': 'decoder.block.10.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.10.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.10.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.10.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.10.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.10.layer.1.dropout',
                        'l_14': 'decoder.block.10.layer.2.layer_norm',
                        'l_15': 'decoder.block.10.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.10.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.10.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.10.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[10]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3689 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3691 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3693 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3840
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3842
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3844
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition35(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:35'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.11.layer.0.layer_norm',
                        'l_1': 'decoder.block.11.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.11.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.11.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.11.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.11.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.11.layer.0.dropout',
                        'l_7': 'decoder.block.11.layer.1.layer_norm',
                        'l_8': 'decoder.block.11.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.11.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.11.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.11.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.11.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.11.layer.1.dropout',
                        'l_14': 'decoder.block.11.layer.2.layer_norm',
                        'l_15': 'decoder.block.11.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.11.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.11.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.11.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[11]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3840 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3842 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3844 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3991
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3993
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3995
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition36(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:36'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.12.layer.0.layer_norm',
                        'l_1': 'decoder.block.12.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.12.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.12.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.12.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.12.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.12.layer.0.dropout',
                        'l_7': 'decoder.block.12.layer.1.layer_norm',
                        'l_8': 'decoder.block.12.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.12.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.12.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.12.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.12.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.12.layer.1.dropout',
                        'l_14': 'decoder.block.12.layer.2.layer_norm',
                        'l_15': 'decoder.block.12.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.12.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.12.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.12.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[12]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3991 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3993 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3995 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4142
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4144
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4146
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition37(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:37'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.13.layer.0.layer_norm',
                        'l_1': 'decoder.block.13.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.13.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.13.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.13.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.13.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.13.layer.0.dropout',
                        'l_7': 'decoder.block.13.layer.1.layer_norm',
                        'l_8': 'decoder.block.13.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.13.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.13.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.13.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.13.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.13.layer.1.dropout',
                        'l_14': 'decoder.block.13.layer.2.layer_norm',
                        'l_15': 'decoder.block.13.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.13.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.13.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.13.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[13]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4142 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4144 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4146 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4293
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4295
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4297
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition38(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:38'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.14.layer.0.layer_norm',
                        'l_1': 'decoder.block.14.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.14.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.14.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.14.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.14.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.14.layer.0.dropout',
                        'l_7': 'decoder.block.14.layer.1.layer_norm',
                        'l_8': 'decoder.block.14.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.14.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.14.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.14.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.14.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.14.layer.1.dropout',
                        'l_14': 'decoder.block.14.layer.2.layer_norm',
                        'l_15': 'decoder.block.14.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.14.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.14.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.14.layer.2.dropout',
                        'l_19': 'decoder.block.15.layer.0.layer_norm',
                        'l_20': 'decoder.block.15.layer.0.SelfAttention.v'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[14]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_20
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4293 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4295 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4297 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = self.l_19(t_5)
        t_1 = t_3[2]
        t_3 = t_3[3]
        t_2 = self.l_20(t_4)
        t_0 = t_4.shape
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4444
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4446
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4448
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::__getattribute___4451
        return list(flatten((x0, t_5, t_4, t_1, t_3, t_2, t_0)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition39(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:39'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.15.layer.0.SelfAttention.q',
                        'l_1': 'decoder.block.15.layer.0.SelfAttention.k',
                        'l_2': 'decoder.block.15.layer.0.SelfAttention.dropout',
                        'l_3': 'decoder.block.15.layer.0.SelfAttention.o',
                        'l_4': 'decoder.block.15.layer.0.dropout',
                        'l_5': 'decoder.block.15.layer.1.layer_norm',
                        'l_6': 'decoder.block.15.layer.1.EncDecAttention.q',
                        'l_7': 'decoder.block.15.layer.1.EncDecAttention.k',
                        'l_8': 'decoder.block.15.layer.1.EncDecAttention.v',
                        'l_9': 'decoder.block.15.layer.1.EncDecAttention.dropout',
                        'l_10': 'decoder.block.15.layer.1.EncDecAttention.o',
                        'l_11': 'decoder.block.15.layer.1.dropout',
                        'l_12': 'decoder.block.15.layer.2.layer_norm',
                        'l_13': 'decoder.block.15.layer.2.DenseReluDense.wi',
                        'l_14': 'decoder.block.15.layer.2.DenseReluDense.dropout',
                        'l_15': 'decoder.block.15.layer.2.DenseReluDense.wo',
                        'l_16': 'decoder.block.15.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4444 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4446 <=> x3
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4448 <=> x4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> x5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Tensor::__getattribute___4451 <=> x6
        x0, x1, x2, x3, x4, x5, x6 = unflatten(args, self.input_structure)
        t_0 = self.l_7(x0)
        t_1 = self.l_8(x0)
        t_2 = self.l_0(x2)
        t_3 = self.l_1(x2)
        t_4 = x6[slice(None, 2, None)]
        t_4 = t_4[0]
        t_2 = t_2.view(t_4, -1, 32, 128)
        t_2 = t_2.transpose(1, 2)
        t_3 = t_3.view(t_4, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_5 = x5.view(t_4, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_3 = t_3.transpose(3, 2)
        t_3 = torch.matmul(t_2, t_3)
        t_3 += x3
        t_2 = t_3.float()
        t_2 = torch.nn.functional.softmax(t_2, dim=-1, _stacklevel=3, dtype=None)
        t_3 = t_2.type_as(t_3)
        t_3 = self.l_2(t_3)
        t_5 = torch.matmul(t_3, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_4 = t_5.view(t_4, -1, 4096)
        t_4 = self.l_3(t_4)
        t_5 = self.l_4(t_4)
        t_5 = x1 + t_5
        t_4 = (t_4, None, x3)
        t_5 = (t_5,)
        t_4 = t_4[slice(1, None, None)]
        t_4 = t_5 + t_4
        t_5 = t_4[slice(None, 2, None)]
        t_3 = t_5[0]
        t_2 = self.l_5(t_3)
        t_5 = t_5[1]
        t_4 = t_4[slice(2, None, None)]
        t_6 = self.l_6(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_6 = t_6.view(t_2, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_2, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_2, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x4
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_9(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_2 = t_1.view(t_2, -1, 4096)
        t_2 = self.l_10(t_2)
        t_1 = self.l_11(t_2)
        t_1 = t_3 + t_1
        t_2 = (t_2, None, x4)
        t_1 = (t_1,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_1 + t_2
        t_1 = t_2[0]
        t_3 = self.l_12(t_1)
        t_2 = t_2[slice(2, None, None)]
        t_3 = self.l_13(t_3)
        t_3 = torch.nn.functional.relu(t_3, inplace=False)
        t_3 = self.l_14(t_3)
        t_3 = self.l_15(t_3)
        t_3 = self.l_16(t_3)
        t_3 = t_1 + t_3
        t_5 = (t_3, t_5)
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/tuple::__getitem___4516
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/tuple::__getitem___4580
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/prim::TupleConstruct_4590
        return list(flatten((x0, t_4, t_2, t_5)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition40(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:40'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, (1,), (1,), (1, 1)]
        self.lookup = {'l_0': 'decoder.block.16.layer.0.layer_norm',
                        'l_1': 'decoder.block.16.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.16.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.16.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.16.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.16.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.16.layer.0.dropout',
                        'l_7': 'decoder.block.16.layer.1.layer_norm',
                        'l_8': 'decoder.block.16.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.16.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.16.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.16.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.16.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.16.layer.1.dropout',
                        'l_14': 'decoder.block.16.layer.2.layer_norm',
                        'l_15': 'decoder.block.16.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.16.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.16.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.16.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[16]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/tuple::__getitem___4516 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/tuple::__getitem___4580 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/prim::TupleConstruct_4590 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = x1 + x2
        t_2 = x3 + t_2
        t_3 = t_2[slice(None, 2, None)]
        t_3 = t_3[0]
        t_4 = self.l_0(t_3)
        t_5 = t_2[2]
        t_2 = t_2[3]
        t_6 = self.l_1(t_4)
        t_7 = self.l_2(t_4)
        t_8 = self.l_3(t_4)
        t_4 = t_4.shape
        t_4 = t_4[slice(None, 2, None)]
        t_4 = t_4[0]
        t_6 = t_6.view(t_4, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_7 = t_7.view(t_4, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_8 = t_8.view(t_4, -1, 32, 128)
        t_8 = t_8.transpose(1, 2)
        t_7 = t_7.transpose(3, 2)
        t_7 = torch.matmul(t_6, t_7)
        t_7 += t_5
        t_6 = t_7.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_7 = t_6.type_as(t_7)
        t_7 = self.l_4(t_7)
        t_8 = torch.matmul(t_7, t_8)
        t_8 = t_8.transpose(1, 2)
        t_8 = t_8.contiguous()
        t_4 = t_8.view(t_4, -1, 4096)
        t_4 = self.l_5(t_4)
        t_8 = self.l_6(t_4)
        t_8 = t_3 + t_8
        t_5 = (t_4, None, t_5)
        t_8 = (t_8,)
        t_5 = t_5[slice(1, None, None)]
        t_5 = t_8 + t_5
        t_8 = t_5[slice(None, 2, None)]
        t_4 = t_8[0]
        t_3 = self.l_7(t_4)
        t_8 = t_8[1]
        t_5 = t_5[slice(2, None, None)]
        t_7 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_7 = t_7.view(t_3, -1, 32, 128)
        t_7 = t_7.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_7, t_0)
        t_0 += t_2
        t_7 = t_0.float()
        t_7 = torch.nn.functional.softmax(t_7, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_7.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_2 = (t_3, None, t_2)
        t_1 = (t_1,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_1 + t_2
        t_1 = t_2[0]
        t_3 = self.l_14(t_1)
        t_2 = t_2[slice(2, None, None)]
        t_2 = t_5 + t_2
        t_3 = self.l_15(t_3)
        t_3 = torch.nn.functional.relu(t_3, inplace=False)
        t_3 = self.l_16(t_3)
        t_3 = self.l_17(t_3)
        t_3 = self.l_18(t_3)
        t_3 = t_1 + t_3
        t_8 = (t_3, t_8)
        t_2 = t_8 + t_2
        t_8 = t_2[slice(None, 2, None)]
        t_8 = t_8[0]
        t_3 = t_2[2]
        t_2 = t_2[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4746
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4748
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4750
        return list(flatten((x0, t_8, t_3, t_2)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition41(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:41'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.17.layer.0.layer_norm',
                        'l_1': 'decoder.block.17.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.17.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.17.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.17.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.17.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.17.layer.0.dropout',
                        'l_7': 'decoder.block.17.layer.1.layer_norm',
                        'l_8': 'decoder.block.17.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.17.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.17.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.17.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.17.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.17.layer.1.dropout',
                        'l_14': 'decoder.block.17.layer.2.layer_norm',
                        'l_15': 'decoder.block.17.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.17.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.17.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.17.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[17]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4746 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4748 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4750 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4897
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4899
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4901
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition42(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:42'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.18.layer.0.layer_norm',
                        'l_1': 'decoder.block.18.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.18.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.18.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.18.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.18.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.18.layer.0.dropout',
                        'l_7': 'decoder.block.18.layer.1.layer_norm',
                        'l_8': 'decoder.block.18.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.18.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.18.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.18.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.18.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.18.layer.1.dropout',
                        'l_14': 'decoder.block.18.layer.2.layer_norm',
                        'l_15': 'decoder.block.18.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.18.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.18.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.18.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[18]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4897 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4899 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4901 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5048
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5050
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5052
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition43(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:43'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.19.layer.0.layer_norm',
                        'l_1': 'decoder.block.19.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.19.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.19.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.19.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.19.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.19.layer.0.dropout',
                        'l_7': 'decoder.block.19.layer.1.layer_norm',
                        'l_8': 'decoder.block.19.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.19.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.19.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.19.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.19.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.19.layer.1.dropout',
                        'l_14': 'decoder.block.19.layer.2.layer_norm',
                        'l_15': 'decoder.block.19.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.19.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.19.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.19.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[19]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5048 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5050 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5052 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5199
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5201
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5203
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition44(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:44'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.20.layer.0.layer_norm',
                        'l_1': 'decoder.block.20.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.20.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.20.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.20.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.20.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.20.layer.0.dropout',
                        'l_7': 'decoder.block.20.layer.1.layer_norm',
                        'l_8': 'decoder.block.20.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.20.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.20.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.20.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.20.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.20.layer.1.dropout',
                        'l_14': 'decoder.block.20.layer.2.layer_norm',
                        'l_15': 'decoder.block.20.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.20.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.20.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.20.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[20]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5199 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5201 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5203 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5350
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5352
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5354
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition45(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:45'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.21.layer.0.layer_norm',
                        'l_1': 'decoder.block.21.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.21.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.21.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.21.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.21.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.21.layer.0.dropout',
                        'l_7': 'decoder.block.21.layer.1.layer_norm',
                        'l_8': 'decoder.block.21.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.21.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.21.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.21.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.21.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.21.layer.1.dropout',
                        'l_14': 'decoder.block.21.layer.2.layer_norm',
                        'l_15': 'decoder.block.21.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.21.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.21.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.21.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[21]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5350 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5352 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5354 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5501
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5503
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5505
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition46(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:46'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.22.layer.0.layer_norm',
                        'l_1': 'decoder.block.22.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.22.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.22.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.22.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.22.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.22.layer.0.dropout',
                        'l_7': 'decoder.block.22.layer.1.layer_norm',
                        'l_8': 'decoder.block.22.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.22.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.22.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.22.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.22.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.22.layer.1.dropout',
                        'l_14': 'decoder.block.22.layer.2.layer_norm',
                        'l_15': 'decoder.block.22.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.22.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.22.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.22.layer.2.dropout'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[22]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5501 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5503 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5505 <=> x3
        x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_5 = t_3[slice(None, 2, None)]
        t_5 = t_5[0]
        t_4 = t_3[2]
        t_3 = t_3[3]
        # Returning:
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout]
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5652
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5654
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5656
        return list(flatten((x0, t_5, t_4, t_3)))

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


class Partition47(nn.Module):
    LAYER_SCOPES = [
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo]',
            'T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout]',
            'T5ForConditionalGeneration/T5Stack[decoder]/T5LayerNorm[final_layer_norm]',
            'T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout]',
            'T5ForConditionalGeneration/Linear[lm_head]',
        ]
    TENSORS = [
        ]
    def __init__(self, layers, tensors, device='cuda:47'):
        super().__init__()

        # Initialize partition layers
        for idx, layer_scope in enumerate(self.LAYER_SCOPES):
            self.add_module(f'l_{idx}' ,layers[layer_scope])

        # Initialize partition tensors (params and buffs)
        b = p = 0
        for tensor_scope in self.TENSORS:
            tensor = tensors[tensor_scope]
            if isinstance(tensor, nn.Parameter):
                self.register_parameter(f'p_{p}', tensor)
                p += 1
            else:
                self.register_buffer(f'b_{b}', tensor)
                b += 1

        self.device = torch.device(device)
        self.input_structure = [1, 1, 1, 1, 1]
        self.lookup = {'l_0': 'decoder.block.23.layer.0.layer_norm',
                        'l_1': 'decoder.block.23.layer.0.SelfAttention.q',
                        'l_2': 'decoder.block.23.layer.0.SelfAttention.k',
                        'l_3': 'decoder.block.23.layer.0.SelfAttention.v',
                        'l_4': 'decoder.block.23.layer.0.SelfAttention.dropout',
                        'l_5': 'decoder.block.23.layer.0.SelfAttention.o',
                        'l_6': 'decoder.block.23.layer.0.dropout',
                        'l_7': 'decoder.block.23.layer.1.layer_norm',
                        'l_8': 'decoder.block.23.layer.1.EncDecAttention.q',
                        'l_9': 'decoder.block.23.layer.1.EncDecAttention.k',
                        'l_10': 'decoder.block.23.layer.1.EncDecAttention.v',
                        'l_11': 'decoder.block.23.layer.1.EncDecAttention.dropout',
                        'l_12': 'decoder.block.23.layer.1.EncDecAttention.o',
                        'l_13': 'decoder.block.23.layer.1.dropout',
                        'l_14': 'decoder.block.23.layer.2.layer_norm',
                        'l_15': 'decoder.block.23.layer.2.DenseReluDense.wi',
                        'l_16': 'decoder.block.23.layer.2.DenseReluDense.dropout',
                        'l_17': 'decoder.block.23.layer.2.DenseReluDense.wo',
                        'l_18': 'decoder.block.23.layer.2.dropout',
                        'l_19': 'decoder.final_layer_norm',
                        'l_20': 'decoder.dropout',
                        'l_21': 'lm_head'}
        self.to(self.device)

    def forward(self, *args):
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5LayerNorm[layer_norm] <=> self.l_0
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[q] <=> self.l_1
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[k] <=> self.l_2
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[v] <=> self.l_3
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Dropout[dropout] <=> self.l_4
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/T5Attention[SelfAttention]/Linear[o] <=> self.l_5
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerSelfAttention[0]/Dropout[dropout] <=> self.l_6
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5LayerNorm[layer_norm] <=> self.l_7
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[q] <=> self.l_8
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[k] <=> self.l_9
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[v] <=> self.l_10
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Dropout[dropout] <=> self.l_11
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/T5Attention[EncDecAttention]/Linear[o] <=> self.l_12
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerCrossAttention[1]/Dropout[dropout] <=> self.l_13
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5LayerNorm[layer_norm] <=> self.l_14
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wi] <=> self.l_15
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Dropout[dropout] <=> self.l_16
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/T5DenseReluDense[DenseReluDense]/Linear[wo] <=> self.l_17
        # T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[23]/ModuleList[layer]/T5LayerFF[2]/Dropout[dropout] <=> self.l_18
        # T5ForConditionalGeneration/T5Stack[decoder]/T5LayerNorm[final_layer_norm] <=> self.l_19
        # T5ForConditionalGeneration/T5Stack[decoder]/Dropout[dropout] <=> self.l_20
        # T5ForConditionalGeneration/Linear[lm_head] <=> self.l_21
        # input4 <=> labels
        # T5ForConditionalGeneration/T5Stack[encoder]/Dropout[dropout] <=> x0
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5652 <=> x1
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5654 <=> x2
        # T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5656 <=> x3
        labels, x0, x1, x2, x3 = unflatten(args, self.input_structure)
        t_0 = self.l_9(x0)
        t_1 = self.l_10(x0)
        t_2 = self.l_0(x1)
        t_3 = self.l_1(t_2)
        t_4 = self.l_2(t_2)
        t_5 = self.l_3(t_2)
        t_2 = t_2.shape
        t_2 = t_2[slice(None, 2, None)]
        t_2 = t_2[0]
        t_3 = t_3.view(t_2, -1, 32, 128)
        t_3 = t_3.transpose(1, 2)
        t_4 = t_4.view(t_2, -1, 32, 128)
        t_4 = t_4.transpose(1, 2)
        t_5 = t_5.view(t_2, -1, 32, 128)
        t_5 = t_5.transpose(1, 2)
        t_4 = t_4.transpose(3, 2)
        t_4 = torch.matmul(t_3, t_4)
        t_4 += x2
        t_3 = t_4.float()
        t_3 = torch.nn.functional.softmax(t_3, dim=-1, _stacklevel=3, dtype=None)
        t_4 = t_3.type_as(t_4)
        t_4 = self.l_4(t_4)
        t_5 = torch.matmul(t_4, t_5)
        t_5 = t_5.transpose(1, 2)
        t_5 = t_5.contiguous()
        t_2 = t_5.view(t_2, -1, 4096)
        t_2 = self.l_5(t_2)
        t_5 = self.l_6(t_2)
        t_5 = x1 + t_5
        t_2 = (t_2, None, x2)
        t_5 = (t_5,)
        t_2 = t_2[slice(1, None, None)]
        t_2 = t_5 + t_2
        t_5 = t_2[slice(None, 2, None)]
        t_4 = t_5[0]
        t_3 = self.l_7(t_4)
        t_5 = t_5[1]
        t_2 = t_2[slice(2, None, None)]
        t_6 = self.l_8(t_3)
        t_3 = t_3.shape
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_6 = t_6.view(t_3, -1, 32, 128)
        t_6 = t_6.transpose(1, 2)
        t_0 = t_0.view(t_3, -1, 32, 128)
        t_0 = t_0.transpose(1, 2)
        t_1 = t_1.view(t_3, -1, 32, 128)
        t_1 = t_1.transpose(1, 2)
        t_0 = t_0.transpose(3, 2)
        t_0 = torch.matmul(t_6, t_0)
        t_0 += x3
        t_6 = t_0.float()
        t_6 = torch.nn.functional.softmax(t_6, dim=-1, _stacklevel=3, dtype=None)
        t_0 = t_6.type_as(t_0)
        t_0 = self.l_11(t_0)
        t_1 = torch.matmul(t_0, t_1)
        t_1 = t_1.transpose(1, 2)
        t_1 = t_1.contiguous()
        t_3 = t_1.view(t_3, -1, 4096)
        t_3 = self.l_12(t_3)
        t_1 = self.l_13(t_3)
        t_1 = t_4 + t_1
        t_3 = (t_3, None, x3)
        t_1 = (t_1,)
        t_3 = t_3[slice(1, None, None)]
        t_3 = t_1 + t_3
        t_1 = t_3[0]
        t_4 = self.l_14(t_1)
        t_3 = t_3[slice(2, None, None)]
        t_3 = t_2 + t_3
        t_4 = self.l_15(t_4)
        t_4 = torch.nn.functional.relu(t_4, inplace=False)
        t_4 = self.l_16(t_4)
        t_4 = self.l_17(t_4)
        t_4 = self.l_18(t_4)
        t_4 = t_1 + t_4
        t_5 = (t_4, t_5)
        t_3 = t_5 + t_3
        t_3 = t_3[slice(None, 2, None)]
        t_3 = t_3[0]
        t_3 = self.l_19(t_3)
        t_3 = self.l_20(t_3)
        t_3 = t_3 * 0.03125
        t_3 = self.l_21(t_3)
        t_5 = t_3.size(-1)
        t_5 = t_3.view(-1, t_5)
        t_3 = labels.view(-1)
        t_3 = torch.nn.functional.cross_entropy(t_5, t_3, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')
        # Returning:
        # T5ForConditionalGeneration/torch.nn.functional::cross_entropy_5820
        return (t_3,)

    def state_dict(self, *args, **kwargs):
        # we return the state dict of this part as it should be in the original model
        return state_dict(self, *args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return load_state_dict(self, *args, **kwargs)

    def named_parameters(self, *args, **kwargs):
        # we return the named parameters of this part as it should be in the original model
        return named_parameters(self, *args, **kwargs)

    def named_buffers(self, *args, **kwargs):
        # we return the named buffers of this part as it should be in the original model
        return named_buffers(self, *args, **kwargs)

    def cpu(self):
        return cpu(self)

    def cuda(self, device=None):
        return cuda(self, device=device)

    def to(self, *args, **kwargs):
        return to(self, *args, **kwargs)


def traverse_model(module: nn.Module, depth: int, prefix: Optional[str] = None,
                   basic_blocks: Tuple[Type[nn.Module]] = (), full: bool = False) -> Iterator[
    Tuple[nn.Module, str, nn.Module, Optional[bool]]]:
    """
    iterate over model layers yielding the layer,layer_scope,encasing_module
    Parameters:
    -----------
    model:
        the model to iterate over
    depth:
        how far down in the model tree to go
    basic_blocks:
        a list of modules that if encountered will not be broken down
    full:
        whether to yield only layers specified by the depth and basic_block options or to yield all layers
    """
    if prefix is None:
        prefix = type(module).__name__

    for name, sub_module in module.named_children():
        scope = prefix + "/" + type(sub_module).__name__ + f"[{name}]"
        if len(list(sub_module.children())) == 0 or isinstance(sub_module, tuple(basic_blocks)) or depth == 0:
            if full:
                # TODO:
                # is_explicit_block_limit = len(list(sub_module.children())) != 0 and (isinstance(sub_module, tuple(basic_blocks)) or depth == 0)
                yield sub_module, scope, module, True

            else:
                yield sub_module, scope, module
        else:
            if full:
                yield sub_module, scope, module, False
            yield from traverse_model(sub_module, depth - 1, scope, basic_blocks, full)


def layerDict(model: nn.Module, depth=1000, basic_blocks=()) -> Dict[str, nn.Module]:
    return {s: l for l, s, _ in traverse_model(model, depth, basic_blocks=basic_blocks)}


def traverse_params_buffs(module: nn.Module, prefix: Optional[str] = None) -> Iterator[Tuple[torch.tensor, str]]:
    """
    iterate over model's buffers and parameters yielding obj,obj_scope

    Parameters:
    -----------
    model:
        the model to iterate over
    """
    if prefix is None:
        prefix = type(module).__name__

    # params
    for param_name, param in module.named_parameters(recurse=False):
        param_scope = f"{prefix}/{type(param).__name__}[{param_name}]"
        yield param, param_scope

    # buffs
    for buffer_name, buffer in module.named_buffers(recurse=False):
        buffer_scope = f"{prefix}/{type(buffer).__name__}[{buffer_name}]"
        yield buffer, buffer_scope

    # recurse
    for name, sub_module in module.named_children():
        yield from traverse_params_buffs(sub_module, prefix + "/" + type(sub_module).__name__ + f"[{name}]")


def tensorDict(model: nn.Module) -> OrderedDict[str, Tensor]:
    return collections.OrderedDict((s, t) for t, s in traverse_params_buffs(model))


def move_tensors(ts, device):
    def move(t):
        if isinstance(t, (nn.Module, Tensor)):
            return t.to(device)
        return t

    return nested_map(move, ts)


def nested_map(func, ts, full=False):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        return func(ts)
    elif isinstance(ts, (list, tuple, set)):
        return type(ts)(nested_map(func, t, full=full) for t in ts)
    elif isinstance(ts, dict):
        return {k: nested_map(func, v, full=full) for k, v in ts.items()}
    elif isinstance(ts, slice) and full:
        start = nested_map(func, ts.start, full=full)
        stop = nested_map(func, ts.stop, full=full)
        step = nested_map(func, ts.step, full=full)
        return slice(start, stop, step)
    return func(ts)


def flatten(ts):
    if isinstance(ts, torch.Size):
        # size is inheriting from tuple which is stupid
        yield ts
    elif isinstance(ts, (list, tuple, set)):
        yield from chain(*[flatten(t) for t in ts])
    elif isinstance(ts, dict):
        yield from chain(*[flatten(t) for k, t in sorted(ts.items(), key=lambda t: t[0])])
    else:
        yield ts


def unflatten(xs, structure):
    return _unflatten(xs, structure)[0]


def _unflatten(xs, structure):
    if isinstance(structure, torch.Size):
        # torch.Size is subclass of tuple which is stupid
        return xs[0], 1

    if not isinstance(structure, (list, tuple, set, dict)):
        return xs[0], 1

    if isinstance(structure, (list, tuple, set)):
        offset = 0
        elements = []
        for s in structure:
            e, n = _unflatten(xs[offset:], s)
            elements.append(e)
            offset += n

        return type(structure)(elements), offset

    assert isinstance(structure, dict)
    offset = 0
    elements = dict()
    for k, v in sorted(structure.items(), key=lambda t: t[0]):
        e, n = _unflatten(xs[offset:], v)
        elements[k] = e
        offset += n

    return elements, offset


def state_dict(partition, *args, **kwargs):
    # we return the state dict of this part as it should be in the original model
    state = nn.Module.state_dict(partition, *args, **kwargs)
    lookup = partition.lookup
    result = dict()
    for k, v in state.items():
        if k in lookup:
            result[lookup[k]] = v
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            result[new_k] = v
    return result


def load_state_dict(partition, state_dict, strict=True):
    reverse_lookup = {v: k for k, v in partition.lookup.items()}
    device = partition.device
    keys = list(partition.state_dict(None).keys())
    new_state = dict()
    for k in keys:
        if k in reverse_lookup:
            new_state[reverse_lookup[k]] = state_dict[k].to(device)
            continue
        idx = k.rfind(".")
        to_replace = k[:idx]
        if to_replace in reverse_lookup:
            key = reverse_lookup[to_replace] + k[idx:]
            new_state[key] = state_dict[k].to(device)
    nn.Module.load_state_dict(partition, new_state, strict=strict)


def named_buffers(partition, prefix='', recurse=True):
    # we return the named buffers of this part as it should be in the original model
    params = nn.Module.named_buffers(partition, prefix=prefix, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield lookup[k], v
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield new_k, v


def named_parameters(partition, prefix='', recurse=True):
    # we return the named parameters of this part as it should be in the original model
    params = nn.Module.named_parameters(partition, prefix=prefix, recurse=recurse)
    lookup = partition.lookup
    for k, v in params:
        if k in lookup:
            yield lookup[k], v
        else:
            assert '.' in k
            split_idx = k.find('.')
            new_k = lookup[k[:split_idx]] + k[split_idx:]
            yield new_k, v


def cpu(partition):
    partition.device = torch.device('cpu')
    return nn.Module.cpu(partition)


def cuda(partition, device=None):
    if device is None:
        device = torch.cuda.current_device()
    partition.device = torch.device(device)
    return nn.Module.cuda(partition, partition.device)


def to(partition, *args, **kwargs):
    device = None
    if 'device' in kwargs:
        device = kwargs['device']
    elif 'tensor' in kwargs:
        device = kwargs['tensor'].device
    if args:
        if isinstance(args[0], (torch.device, int, str)):
            device = args[0]
        if torch.is_tensor(args[0]):
            device = args[0].device
    if not (device is None):
        partition.device = torch.device(device)
    return nn.Module.to(partition, *args, **kwargs)

model_args = {'model_name_or_path': 't5-3b', 'max_seq_length': 320, 'answer_max_seq_length': 8, 'stateless_tied': True, 'lmhead': True, 'precompute_masks': False}

def t53b32gpu_t5_3b_tied_lmheads_320_8_32p_bw12_async_squad1_mpipe():
    return dict(model_type='t5_stateless',
                model_name_or_path='t5-3b',
                do_lower_case=False,
                output_past=False,
                stateless_tied=True,
                explicitly_set_dict={'return_dict': False, 'use_cache': False, 'output_only': True, 'output_attentions': False, 'precompute_masks': False, 'output_hidden_states': False},
                do_resize_token_embedding=True,
                )
    
"""analysis summary
-I- Printing Report
warnings:
Partition0 output:T5ForConditionalGeneration/T5Stack[encoder]/ModuleList[block]/T5Block[0]/tuple::__getitem___136_0 is not contiguous!
Partition1 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___236 is not contiguous!
Partition2 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___320 is not contiguous!
Partition3 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___404 is not contiguous!
Partition4 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___488 is not contiguous!
Partition5 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___572 is not contiguous!
Partition6 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___656 is not contiguous!
Partition7 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___740 is not contiguous!
Partition8 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___824 is not contiguous!
Partition9 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___908 is not contiguous!
Partition10 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___992 is not contiguous!
Partition11 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1076 is not contiguous!
Partition12 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1160 is not contiguous!
Partition13 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1244 is not contiguous!
Partition14 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1328 is not contiguous!
Partition15 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1412 is not contiguous!
Partition16 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1496 is not contiguous!
Partition17 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1580 is not contiguous!
Partition18 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1664 is not contiguous!
Partition19 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1748 is not contiguous!
Partition20 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1832 is not contiguous!
Partition21 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___1916 is not contiguous!
Partition22 output:T5ForConditionalGeneration/T5Stack[encoder]/tuple::__getitem___2000 is not contiguous!
Partition24 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2332 is not contiguous!
Partition25 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2483 is not contiguous!
Partition26 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2634 is not contiguous!
Partition27 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2785 is not contiguous!
Partition28 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___2936 is not contiguous!
Partition29 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3087 is not contiguous!
Partition30 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3238 is not contiguous!
Partition31 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3389 is not contiguous!
Partition32 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3540 is not contiguous!
Partition33 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3691 is not contiguous!
Partition34 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3842 is not contiguous!
Partition35 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___3993 is not contiguous!
Partition36 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4144 is not contiguous!
Partition37 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4295 is not contiguous!
Partition38 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4446 is not contiguous!
Partition39 output:T5ForConditionalGeneration/T5Stack[decoder]/ModuleList[block]/T5Block[15]/tuple::__getitem___4516_0 is not contiguous!
Partition40 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4748 is not contiguous!
Partition41 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___4899 is not contiguous!
Partition42 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5050 is not contiguous!
Partition43 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5201 is not contiguous!
Partition44 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5352 is not contiguous!
Partition45 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5503 is not contiguous!
Partition46 output:T5ForConditionalGeneration/T5Stack[decoder]/tuple::__getitem___5654 is not contiguous!
Number of nodes in Computation Graph: 5821
Number of stages: 32
n_partitions:48, num_dummy_stages:16
unique_stages_on_same_gpu: [{0, 39}, {1}, {2}, {3}, {4}, {5}, {27, 6}, {33, 7}, {8}, {9}, {32, 10}, {34, 11}, {12, 46}, {28, 13}, {44, 14}, {15}, {16, 31}, {17, 29}, {41, 18}, {19, 37}, {26, 20}, {43, 21}, {25, 22}, {30, 23}, {24}, {35}, {36}, {38}, {40}, {42}, {45}, {47}]
"stage_to_device_map": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 22, 20, 6, 13, 17, 23, 16, 10, 7, 11, 25, 26, 19, 27, 0, 28, 18, 29, 21, 14, 30, 12, 31],
backward times include recomputation
Analysis for async_pipeline=True: last partition will not do recomputation.

Stage parameter count:
 {0: 83205120,
 1: 50333696,
 2: 50333696,
 3: 50333696,
 4: 50333696,
 5: 50333696,
 6: 50333696,
 7: 50333696,
 8: 50333696,
 9: 50333696,
 10: 50333696,
 11: 50333696,
 12: 50333696,
 13: 50333696,
 14: 50333696,
 15: 50333696,
 16: 50333696,
 17: 50333696,
 18: 50333696,
 19: 50333696,
 20: 50333696,
 21: 50333696,
 22: 50333696,
 23: 50334720,
 24: 67112960,
 25: 67111936,
 26: 67111936,
 27: 67111936,
 28: 67111936,
 29: 67111936,
 30: 67111936,
 31: 67111936,
 32: 67111936,
 33: 67111936,
 34: 67111936,
 35: 67111936,
 36: 67111936,
 37: 67111936,
 38: 71307264,
 39: 62916608,
 40: 67111936,
 41: 67111936,
 42: 67111936,
 43: 67111936,
 44: 67111936,
 45: 67111936,
 46: 67111936,
 47: 99983360,
 'total': 2884440064}

GPU parameter count:
 Number of Model Parameters 2884.4M
{0: 146121728,
 1: 50333696,
 2: 50333696,
 3: 50333696,
 4: 50333696,
 5: 50333696,
 6: 117445632,
 7: 117445632,
 8: 50333696,
 9: 50333696,
 10: 117445632,
 11: 117445632,
 12: 117445632,
 13: 117445632,
 14: 117445632,
 15: 50333696,
 16: 117445632,
 17: 117445632,
 18: 117445632,
 19: 117445632,
 20: 117445632,
 21: 117445632,
 22: 117445632,
 23: 117446656,
 24: 67112960,
 35: 67111936,
 36: 67111936,
 38: 71307264,
 40: 67111936,
 42: 67111936,
 45: 67111936,
 47: 99983360,
 'total': 2884440064}

real times are based on real measurements of execution time (with communication) of generated partitions ms
forward {0: 32.55, 1: 17.34, 2: 17.97, 3: 17.71, 4: 17.49, 5: 17.28, 6: 21.77, 7: 21.36, 8: 17.43, 9: 17.48, 10: 21.67, 11: 21.63, 12: 21.86, 13: 21.81, 14: 21.64, 15: 17.48, 16: 21.89, 17: 21.53, 18: 21.62, 19: 21.35, 20: 21.99, 21: 21.58, 22: 22.19, 23: 15.16, 24: 2.42, 35: 4.13, 36: 3.85, 38: 4.23, 40: 3.98, 42: 3.99, 45: 4.02, 47: 3.2}
backward {0: 48.79, 1: 32.61, 2: 34.73, 3: 33.22, 4: 33.93, 5: 32.83, 6: 39.2, 7: 39.71, 8: 33.34, 9: 34.14, 10: 39.28, 11: 41.99, 12: 41.02, 13: 39.19, 14: 40.42, 15: 34.09, 16: 41.15, 17: 41.75, 18: 40.89, 19: 40.29, 20: 41.01, 21: 39.8, 22: 41.15, 23: 42.74, 24: 17.73, 35: 6.68, 36: 6.64, 38: 9.4, 40: 6.7, 42: 6.79, 45: 6.68, 47: 6.66}

Analysis for T = (1-R)fwd + R*bwd:

Pipeline Slowdown: (compared to sequential execution with no communication, and same recompute policy)
forward 3.962
backward 2.122

Expected utilization by partition
forward {0: 0.33, 1: 0.24, 2: 0.26, 3: 0.25, 4: 0.24, 5: 0.24, 6: 0.34, 7: 0.33, 8: 0.24, 9: 0.24, 10: 0.34, 11: 0.33, 12: 0.34, 13: 0.34, 14: 0.34, 15: 0.24, 16: 0.34, 17: 0.33, 18: 0.33, 19: 0.33, 20: 0.35, 21: 0.33, 22: 0.35, 23: 0.41, 24: 0.04, 35: 0.09, 36: 0.09, 38: 0.09, 40: 0.09, 42: 0.09, 45: 0.09, 47: 0.1}
backward {0: 0.97, 1: 0.47, 2: 0.51, 3: 0.48, 4: 0.5, 5: 0.47, 6: 0.58, 7: 0.59, 8: 0.48, 9: 0.5, 10: 0.58, 11: 0.64, 12: 0.62, 13: 0.58, 14: 0.61, 15: 0.5, 16: 0.62, 17: 0.64, 18: 0.62, 19: 0.61, 20: 0.62, 21: 0.6, 22: 0.62, 23: 0.66, 24: 0.12, 35: 0.12, 36: 0.12, 38: 0.17, 40: 0.12, 42: 0.12, 45: 0.12, 47: 0.12}

worstcase: bwd: 48.792 fwd: 32.546
Expected speedup for 32 partitions is: 11.196
Assuming bandwidth of 12 GBps between GPUs

communication volumes size of activations of each partition
0: input size:'14.75 MB', recieve_time:'1.23 ms', out:'260.26 MB', send time:'21.69 ms'
1: input size:'115.34 MB', recieve_time:'9.61 ms', out:'115.34 MB', send time:'9.61 ms'
2: input size:'115.34 MB', recieve_time:'9.61 ms', out:'115.34 MB', send time:'9.61 ms'
3: input size:'115.34 MB', recieve_time:'9.61 ms', out:'115.34 MB', send time:'9.61 ms'
4: input size:'115.34 MB', recieve_time:'9.61 ms', out:'115.34 MB', send time:'9.61 ms'
5: input size:'115.34 MB', recieve_time:'9.61 ms', out:'115.34 MB', send time:'9.61 ms'
6: input size:'128.78 MB', recieve_time:'10.73 ms', out:'128.78 MB', send time:'10.73 ms'
7: input size:'128.78 MB', recieve_time:'10.73 ms', out:'128.78 MB', send time:'10.73 ms'
8: input size:'115.34 MB', recieve_time:'9.61 ms', out:'115.34 MB', send time:'9.61 ms'
9: input size:'115.34 MB', recieve_time:'9.61 ms', out:'115.34 MB', send time:'9.61 ms'
10: input size:'128.78 MB', recieve_time:'10.73 ms', out:'128.78 MB', send time:'10.73 ms'
11: input size:'128.78 MB', recieve_time:'10.73 ms', out:'128.78 MB', send time:'10.73 ms'
12: input size:'128.78 MB', recieve_time:'10.73 ms', out:'128.78 MB', send time:'10.73 ms'
13: input size:'128.78 MB', recieve_time:'10.73 ms', out:'128.78 MB', send time:'10.73 ms'
14: input size:'128.78 MB', recieve_time:'10.73 ms', out:'128.78 MB', send time:'10.73 ms'
15: input size:'115.34 MB', recieve_time:'9.61 ms', out:'115.34 MB', send time:'9.61 ms'
16: input size:'128.78 MB', recieve_time:'10.73 ms', out:'128.78 MB', send time:'10.73 ms'
17: input size:'128.78 MB', recieve_time:'10.73 ms', out:'128.78 MB', send time:'10.73 ms'
18: input size:'128.78 MB', recieve_time:'10.73 ms', out:'128.78 MB', send time:'10.73 ms'
19: input size:'128.78 MB', recieve_time:'10.73 ms', out:'128.78 MB', send time:'10.73 ms'
20: input size:'128.78 MB', recieve_time:'10.73 ms', out:'128.78 MB', send time:'10.73 ms'
21: input size:'128.78 MB', recieve_time:'10.73 ms', out:'128.78 MB', send time:'10.73 ms'
22: input size:'128.78 MB', recieve_time:'10.73 ms', out:'128.78 MB', send time:'10.73 ms'
23: input size:'128.78 MB', recieve_time:'10.73 ms', out:'23.92 MB', send time:'1.99 ms'
24: input size:'141.97 MB', recieve_time:'11.83 ms', out:'13.43 MB', send time:'1.12 ms'
35: input size:'13.43 MB', recieve_time:'1.12 ms', out:'13.43 MB', send time:'1.12 ms'
36: input size:'13.43 MB', recieve_time:'1.12 ms', out:'13.43 MB', send time:'1.12 ms'
38: input size:'13.43 MB', recieve_time:'1.12 ms', out:'14.75 MB', send time:'1.23 ms'
40: input size:'13.43 MB', recieve_time:'1.12 ms', out:'13.43 MB', send time:'1.12 ms'
42: input size:'13.43 MB', recieve_time:'1.12 ms', out:'13.43 MB', send time:'1.12 ms'
45: input size:'13.43 MB', recieve_time:'1.12 ms', out:'13.43 MB', send time:'1.12 ms'
47: input size:'13.43 MB', recieve_time:'1.12 ms', out:'0.00 MB', send time:'0.00 ms'

Compuatation Communication ratio (comp/(comp+comm)):
forward {0: 0.33, 1: 0.45, 2: 0.47, 3: 0.46, 4: 0.45, 5: 0.44, 6: 0.51, 7: 0.5, 8: 0.45, 9: 0.45, 10: 0.5, 11: 0.5, 12: 0.51, 13: 0.51, 14: 0.5, 15: 0.45, 16: 0.51, 17: 0.5, 18: 0.5, 19: 0.5, 20: 0.51, 21: 0.5, 22: 0.52, 23: 0.87, 24: 0.54, 35: 0.73, 36: 0.71, 38: 0.71, 40: 0.72, 42: 0.72, 45: 0.72, 47: 1.0} 
backward {0: 0.97, 1: 0.71, 2: 0.72, 3: 0.71, 4: 0.72, 5: 0.71, 6: 0.73, 7: 0.73, 8: 0.71, 9: 0.72, 10: 0.73, 11: 0.74, 12: 0.74, 13: 0.73, 14: 0.73, 15: 0.72, 16: 0.74, 17: 0.74, 18: 0.74, 19: 0.73, 20: 0.74, 21: 0.73, 22: 0.74, 23: 0.75, 24: 0.33, 35: 0.83, 36: 0.83, 38: 0.88, 40: 0.83, 42: 0.84, 45: 0.83, 47: 0.83}

Analysis for T = fwd + bwd:
 {'expected_compute_utilization': {0: 1.0,
                                  1: 0.53,
                                  2: 0.57,
                                  3: 0.54,
                                  4: 0.55,
                                  5: 0.53,
                                  6: 0.68,
                                  7: 0.68,
                                  8: 0.54,
                                  9: 0.55,
                                  10: 0.68,
                                  11: 0.72,
                                  12: 0.71,
                                  13: 0.68,
                                  14: 0.69,
                                  15: 0.55,
                                  16: 0.71,
                                  17: 0.72,
                                  18: 0.7,
                                  19: 0.69,
                                  20: 0.71,
                                  21: 0.68,
                                  22: 0.72,
                                  23: 0.77,
                                  24: 0.12,
                                  35: 0.15,
                                  36: 0.14,
                                  38: 0.19,
                                  40: 0.14,
                                  42: 0.15,
                                  45: 0.14,
                                  47: 0.15},
 'pipeline_no_comm': {0: 58.42,
                      1: 30.73,
                      2: 33.48,
                      3: 31.71,
                      4: 32.2,
                      5: 30.89,
                      6: 39.51,
                      7: 39.61,
                      8: 31.55,
                      9: 32.4,
                      10: 39.49,
                      11: 42.16,
                      12: 41.41,
                      13: 39.54,
                      14: 40.6,
                      15: 32.34,
                      16: 41.57,
                      17: 41.82,
                      18: 41.04,
                      19: 40.18,
                      20: 41.54,
                      21: 39.91,
                      22: 41.87,
                      23: 45.18,
                      24: 7.21,
                      35: 8.57,
                      36: 8.26,
                      38: 11.28,
                      40: 8.44,
                      42: 8.54,
                      45: 8.47,
                      47: 8.74,
                      'worstcase': 58.42},
 'pipeline_vs_seq_no_comm': 13.4,
 'pipeline_with_non_parallel_comm': {0: 81.34,
                                     1: 49.95,
                                     2: 52.7,
                                     3: 50.93,
                                     4: 51.43,
                                     5: 50.11,
                                     6: 60.98,
                                     7: 61.07,
                                     8: 50.78,
                                     9: 51.62,
                                     10: 60.95,
                                     11: 63.62,
                                     12: 62.87,
                                     13: 61.0,
                                     14: 62.06,
                                     15: 51.56,
                                     16: 63.04,
                                     17: 63.28,
                                     18: 62.5,
                                     19: 61.64,
                                     20: 63.0,
                                     21: 61.38,
                                     22: 63.33,
                                     23: 57.9,
                                     24: 20.16,
                                     35: 10.81,
                                     36: 10.5,
                                     38: 13.63,
                                     40: 10.68,
                                     42: 10.78,
                                     45: 10.71,
                                     47: 9.86,
                                     'worstcase': 81.34},
 'seq_no_comm_no_recomp': {0: 48.91,
                           1: 23.12,
                           2: 23.63,
                           3: 23.25,
                           4: 24.55,
                           5: 23.63,
                           6: 23.31,
                           7: 23.58,
                           8: 23.8,
                           9: 23.86,
                           10: 23.58,
                           11: 23.54,
                           12: 23.96,
                           13: 24.09,
                           14: 23.46,
                           15: 24.11,
                           16: 23.72,
                           17: 23.65,
                           18: 23.75,
                           19: 24.04,
                           20: 23.99,
                           21: 23.77,
                           22: 23.57,
                           23: 27.2,
                           24: 5.79,
                           25: 7.92,
                           26: 7.79,
                           27: 7.95,
                           28: 7.92,
                           29: 7.9,
                           30: 7.81,
                           31: 7.98,
                           32: 7.98,
                           33: 7.69,
                           34: 8.0,
                           35: 7.7,
                           36: 7.71,
                           37: 7.93,
                           38: 8.13,
                           39: 5.18,
                           40: 7.88,
                           41: 7.34,
                           42: 7.82,
                           43: 7.73,
                           44: 7.89,
                           45: 7.59,
                           46: 7.78,
                           47: 9.3}}

expected_speedup_compared_to_seq_no_recomp_no_comm: 9.624
Analysis max cuda memory used 1.90GB
"""